{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the dataset and in read_csv, encoding has been used to decode the file as there are some characters that are compatible with the provided encoding\n",
    "df = pd.read_csv(\"/Users/User/Downloads/Sentiment_Data.csv\", encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@_angelica_toy Happy Anniversary!!!....The Day...</td>\n",
       "      <td>Mild_Pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@McfarlaneGlenda Happy Anniversary!!!....The D...</td>\n",
       "      <td>Mild_Pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@thevivafrei @JustinTrudeau Happy Anniversary!...</td>\n",
       "      <td>Mild_Pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@NChartierET Happy Anniversary!!!....The Day t...</td>\n",
       "      <td>Mild_Pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@tabithapeters05 Happy Anniversary!!!....The D...</td>\n",
       "      <td>Mild_Pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet Sentiment\n",
       "0  @_angelica_toy Happy Anniversary!!!....The Day...  Mild_Pos\n",
       "1  @McfarlaneGlenda Happy Anniversary!!!....The D...  Mild_Pos\n",
       "2  @thevivafrei @JustinTrudeau Happy Anniversary!...  Mild_Pos\n",
       "3  @NChartierET Happy Anniversary!!!....The Day t...  Mild_Pos\n",
       "4  @tabithapeters05 Happy Anniversary!!!....The D...  Mild_Pos"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 451332 entries, 0 to 451331\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   Tweet      451331 non-null  object\n",
      " 1   Sentiment  451332 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 6.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Rows based on the 'Tweet' column:\n",
      "      Tweet Sentiment\n",
      "75986   NaN   Neutral\n"
     ]
    }
   ],
   "source": [
    "# Identify the empty rows\n",
    "empty_rows = df[df['Tweet'].isnull()]\n",
    "print(\"Empty Rows based on the 'Tweet' column:\")\n",
    "print(empty_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The above row doesn't have any tweet and just having a value on Sentiment which doesn't provide us any useful information that why the sentiment is consisdered as Neutral. So, it's better to remove this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the row\n",
    "df.drop(75986,axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(451331, 2)"
      ]
     },
     "execution_count": 542,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### While exploring the data, in few rows we encountered with \"#NAME?\" which is not useful in our sentiment analysis, so we are dropping these rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows with '#NAME?': 5\n"
     ]
    }
   ],
   "source": [
    "# Count the number of rows with \"#NAME?\" in the \"Tweet\" column\n",
    "num_name_rows=(df['Tweet']=='#NAME?').sum()\n",
    "print(\"Number of Rows with '#NAME?':\", num_name_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>#NAME?</td>\n",
       "      <td>Mild_Pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34287</th>\n",
       "      <td>#NAME?</td>\n",
       "      <td>Strong_Pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284517</th>\n",
       "      <td>#NAME?</td>\n",
       "      <td>Strong_Pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318448</th>\n",
       "      <td>#NAME?</td>\n",
       "      <td>Strong_Pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407118</th>\n",
       "      <td>#NAME?</td>\n",
       "      <td>Strong_Pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Tweet   Sentiment\n",
       "1029    #NAME?    Mild_Pos\n",
       "34287   #NAME?  Strong_Pos\n",
       "284517  #NAME?  Strong_Pos\n",
       "318448  #NAME?  Strong_Pos\n",
       "407118  #NAME?  Strong_Pos"
      ]
     },
     "execution_count": 545,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_rows=df[df['Tweet'] == '#NAME?']\n",
    "name_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with \"#NAME?\" in the \"Tweet\" column inplace\n",
    "df.drop(df[df['Tweet'] == '#NAME?'].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(451326, 2)"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Duplicate Rows: 33\n"
     ]
    }
   ],
   "source": [
    "# Count the number of duplicate rows\n",
    "num_duplicate_rows = df.duplicated().sum()\n",
    "print(\"Number of Duplicate Rows:\", num_duplicate_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    Tweet   Sentiment\n",
      "4374    @unacceptfringe @thevivafrei @ShadoeDavis @Tru...    Mild_Pos\n",
      "9471    @thelifeofalvo @usher933 @Baba_Ganoushy @Huddy...  Strong_Pos\n",
      "14756   @TinfoilhatNick @WarcampaignYT @rkpall @PanSoy...  Strong_Pos\n",
      "18440   @kylegriffin1 You all were behind vaccine pass...  Strong_Pos\n",
      "29341   @WeAreCanProud If you support the demonstratio...  Strong_Pos\n",
      "48723   @NChartierET Deering &amp; Braun haven't filed...  Strong_Pos\n",
      "58390   @Brent_Secord @JasonLavigneMP @NadineGNess @rr...  Strong_Pos\n",
      "61472   @acoyne Two-thirds of Canadians support use of...  Strong_Pos\n",
      "61513   @brianlilley Two-thirds of Canadians support u...  Strong_Pos\n",
      "70312   @nationalpost Two-thirds of Canadians support ...  Strong_Pos\n",
      "73873   @globeandmail Two-thirds of Canadians support ...  Strong_Pos\n",
      "74580   @Qualifyfor @Derricktgoat ðââï¸The Win...  Strong_Pos\n",
      "77729   @MattWalshBlog Quoting #MattWalsh's own words ...    Mild_Pos\n",
      "78596   @CTVNews The only law that the Freedom Convoy ...  Strong_Pos\n",
      "78997   @Bill096409832 @OnlyHumanFolks @freeandwildCA ...  Strong_Pos\n",
      "145842  @Laurel_BC the Freedom Convoy protest was not ...     Neutral\n",
      "148128  @CTVNews When Poilievre said he supported the ...  Strong_Pos\n",
      "151090  @ctvottawa When Poilievre said he supported th...  Strong_Pos\n",
      "161551  @ctvottawa When Poilievre said he supported th...  Strong_Pos\n",
      "165184  @ZTPetrizzo Dear Peoples Convoyâ¦ your leader...  Strong_Pos\n",
      "229812  @borngeek @GlennCarr6 @wlbeeton @Kenneth727129...  Strong_Pos\n",
      "232070  @JustinTrudeau Leading Israeli physicians and ...    Mild_Pos\n",
      "233562  @JustinTrudeau Leading Israeli physicians and ...    Mild_Pos\n",
      "243797  @HoaNguy01811981 @_Desi76_ @bombaybadboy @stan...  Strong_Pos\n",
      "244940  @Justin_Ling @MiltonCPC_EDA @CPCPontiac @hq_cp...  Strong_Neg\n",
      "284509  @josepharthur You are brainwashed. You need he...  Strong_Pos\n",
      "319711  @RebeccaWarnefo3 @Angelscu2 @MatthewTrott3 @Ca...  Strong_Pos\n",
      "331799  @tylercowen  The Canadian âFreedom Convoyâ...    Mild_Neg\n",
      "347129  @CPHO_Canada Freedom Convoy Organizers:\\n\"We a...     Neutral\n",
      "347994  @jannarden This is very concerning. The Organi...  Strong_Pos\n",
      "348018  @jannarden This is very concerning. The Organi...  Strong_Pos\n",
      "361003  @wengcouver @Kmcall1989 @jonkay @itsrobroberts...    Mild_Neg\n",
      "385921  @TheoFleury14 @WBrettWilson @RebelNewsOnline @...     Neutral\n"
     ]
    }
   ],
   "source": [
    "#checking duplicates\n",
    "duplicate_rows=df[df.duplicated()]\n",
    "print(duplicate_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After doing the research on keeping the duplicates or removing them, we would consider to remove them as we want to ensure more robust and unbaised model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing duplicate rows\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment distribution:\n",
      "Sentiment\n",
      "Strong_Pos    233673\n",
      "Neutral        77012\n",
      "Mild_Pos       63999\n",
      "Strong_Neg     42555\n",
      "Mild_Neg       34054\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the distribution of the sentiment column\n",
    "print(\"Sentiment distribution:\")\n",
    "print(df['Sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing patterns, mentions, url's, special characters, numbers and punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes pattern in the input text\n",
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for word in r:\n",
    "        input_txt = re.sub(word, \"\", input_txt)\n",
    "    return input_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing mentions\n",
    "df['clean_tweet'] = df['Tweet'].apply(lambda x: remove_pattern(x, \"@[\\w]*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "# Apply the function to remove URLs\n",
    "df['clean_tweet'] = df['clean_tweet'].apply(remove_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing special characters, numbers and punctuations\n",
    "df['clean_tweet'] = df['clean_tweet'].str.replace(\"[^a-zA-Z#]\", \" \", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordninja\n",
    "# Function to clean tweet and split hashtags\n",
    "def clean_tweet(tweet):\n",
    "    # Split the tweet into words\n",
    "    words = tweet.split()\n",
    "    \n",
    "    # Initialize a list to hold the cleaned words\n",
    "    cleaned_words = []\n",
    "    \n",
    "    # Iterate through each word\n",
    "    for word in words:\n",
    "        # If the word is a hashtag\n",
    "        if word.startswith('#'):\n",
    "            # Remove the hash symbol and split the hashtag into words\n",
    "            split_words = wordninja.split(word[1:])\n",
    "            # Add the split words to the cleaned words list\n",
    "            cleaned_words.extend(split_words)\n",
    "        else:\n",
    "            # Add the word as is to the cleaned words list\n",
    "            cleaned_words.append(word)\n",
    "    \n",
    "    return ' '.join(cleaned_words)\n",
    "\n",
    "# Apply the clean_tweet function to the 'clean_tweet' column\n",
    "df['clean_tweet'] = df['clean_tweet'].apply(clean_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting into lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to lowercase\n",
    "df['clean_tweet'] = df['clean_tweet'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    Tweet   Sentiment  \\\n",
      "0       @_angelica_toy Happy Anniversary!!!....The Day...    Mild_Pos   \n",
      "1       @McfarlaneGlenda Happy Anniversary!!!....The D...    Mild_Pos   \n",
      "2       @thevivafrei @JustinTrudeau Happy Anniversary!...    Mild_Pos   \n",
      "3       @NChartierET Happy Anniversary!!!....The Day t...    Mild_Pos   \n",
      "4       @tabithapeters05 Happy Anniversary!!!....The D...    Mild_Pos   \n",
      "...                                                   ...         ...   \n",
      "451327  Gaza; Peace n' Freedom - Viva Palestina convoy...  Strong_Pos   \n",
      "451328  Face of Defense: Soldier Finds Freedom in U.S....  Strong_Pos   \n",
      "451329  Face of Defense: Soldier Finds Freedom in U.S....  Strong_Pos   \n",
      "451330  Gaza; Peace n' Freedom - \"Israel stops aid con...  Strong_Pos   \n",
      "451331             @convoy 83 yes! get on freedom server!  Strong_Pos   \n",
      "\n",
      "                                              clean_tweet  \n",
      "0       happy anniversary the day the freedumb died in...  \n",
      "1       happy anniversary the day the freedumb died in...  \n",
      "2       happy anniversary the day the freedumb died in...  \n",
      "3       happy anniversary the day the freedumb died in...  \n",
      "4       happy anniversary the day the freedumb died in...  \n",
      "...                                                   ...  \n",
      "451327  gaza peace n freedom viva palestina convoy ent...  \n",
      "451328  face of defense soldier finds freedom in u s f...  \n",
      "451329  face of defense soldier finds freedom in u s f...  \n",
      "451330  gaza peace n freedom israel stops aid convoy e...  \n",
      "451331                          yes get on freedom server  \n",
      "\n",
      "[451293 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame containing the 'clean_tweet' column\n",
    "\n",
    "# Remove 'rt' from the sentences\n",
    "df['clean_tweet'] = df['clean_tweet'].str.replace(r'\\brt\\b', '', regex=True)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for repeated words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize repeated characters\n",
    "def normalize_repeated_characters(text):\n",
    "    return re.sub(r'^(?!.*(.)\\1).{4,10}$', text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing Slang Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "slang_dict = {\n",
    "    'brb': 'be right back',\n",
    "    'btw': 'by the way',\n",
    "    'idk': \"I don't know\",\n",
    "    'lol': 'laugh out loud',\n",
    "    'omg': 'oh my god',\n",
    "    'ttyl': 'talk to you later',\n",
    "    'u': 'you',\n",
    "    'ur': 'you are',\n",
    "    'lmk': 'let me know',\n",
    "    'smh': 'shaking my head',\n",
    "    'tbh': 'to be honest',\n",
    "    'rofl': 'rolling on the floor laughing',\n",
    "    'wtf': 'what the f***',\n",
    "    'bff': 'best friends forever',\n",
    "    'fyi': 'for your information',\n",
    "    'jk': 'just kidding',\n",
    "    'np': 'no problem',\n",
    "    'omw': 'on my way',\n",
    "    'rn': 'right now',\n",
    "    'thx': 'thanks',\n",
    "    'afaik': 'as far as I know',\n",
    "    'b4': 'before',\n",
    "    'cya': 'see you',\n",
    "    'gr8': 'great',\n",
    "    'msg': 'message',\n",
    "    'nvm': 'never mind',\n",
    "    'plz': 'please',\n",
    "    'sry': 'sorry',\n",
    "    'w/': 'with',\n",
    "    'w/o': 'without',\n",
    "    'yolo': 'you only live once',\n",
    "    'wya': 'where you at',\n",
    "    'fomo': 'fear of missing out',\n",
    "    'ikr': 'I know, right?',\n",
    "    'imho': 'in my humble opinion',\n",
    "    'irl': 'in real life',\n",
    "    'lit': 'exciting or excellent',\n",
    "    'hmu': 'hit me up',\n",
    "    'bae': 'before anyone else',\n",
    "    'n': 'and',\n",
    "    'r': 'are',\n",
    "    'b': 'with',\n",
    "    'z': 'the',\n",
    "    'th': 'the',\n",
    "    'int': 'international',\n",
    "    'tha': 'that',\n",
    "    'ppl': 'people',\n",
    "    'cdn': 'canadians',\n",
    "    'jan': 'january'\n",
    "}\n",
    "# Function to replace slang words using the slang dictionary\n",
    "def replace_slang(text, slang_dict):\n",
    "    words = text.split()\n",
    "    replaced_words = [slang_dict[word.lower()] if word.lower() in slang_dict else word for word in words]\n",
    "    return \" \".join(replaced_words)\n",
    "\n",
    "# Apply replace_slang function to a DataFrame column\n",
    "df['clean_tweet'] = df['clean_tweet'].apply(lambda x: replace_slang(x, slang_dict))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping Contractions to Expanded Forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary mapping contractions to their expanded forms\n",
    "contractions_dict = {\n",
    "    \"ain't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"who'd\": \"who would\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who're\": \"who are\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "# Function to expand contractions in a given text using the contractions dictionary\n",
    "def expand_contractions(text, contractions_dict):\n",
    "    # Regular expression pattern to find contractions in text\n",
    "    pattern = re.compile(r'\\b(' + '|'.join(contractions_dict.keys()) + r')\\b')\n",
    "\n",
    "    # Function to expand a matched contraction using the dictionary\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        expanded = contractions_dict.get(match)\n",
    "        if not expanded:\n",
    "            expanded = contractions_dict.get(match.lower())\n",
    "        return expanded\n",
    "\n",
    "    # Apply the contraction expansion function to the text\n",
    "    expanded_text = pattern.sub(expand_match, text)\n",
    "    return expanded_text\n",
    "\n",
    "# Apply contraction expansion to all cells of the DataFrame\n",
    "df['clean_tweet'] = df['clean_tweet'].apply(lambda x: expand_contractions(x, contractions_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling negatations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to remove _neg suffix from each word in a string\n",
    "def remove_neg_suffix(text):\n",
    "    return ' '.join([word[:-4] if word.endswith('_neg') else word for word in text.split()])\n",
    "\n",
    "# Apply the function to the 'clean_tweet' column\n",
    "df['clean_tweet'] = df['clean_tweet'].apply(remove_neg_suffix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking duplicates again as after cleaning some tweets are retweeted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Duplicate Rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Count the number of duplicate rows\n",
    "num_duplicate_rows = df.duplicated().sum()\n",
    "print(\"Number of Duplicate Rows:\", num_duplicate_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Tweet, Sentiment, clean_tweet]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#checking duplicates\n",
    "duplicate_rows=df[df.duplicated()]\n",
    "print(duplicate_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing duplicate rows\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(451293, 3)"
      ]
     },
     "execution_count": 566,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we are checking the duplicates together for 'clean_tweet' and 'Sentiment' columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 77404\n"
     ]
    }
   ],
   "source": [
    "# Count the number of duplicate rows based on 'clean_tweet' and 'Sentiment' columns\n",
    "num_duplicate_rows = df.duplicated(subset=['clean_tweet', 'Sentiment']).sum()\n",
    "\n",
    "# Display the number of duplicate rows\n",
    "print(\"Number of duplicate rows:\", num_duplicate_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    Tweet   Sentiment  \\\n",
      "0       @_angelica_toy Happy Anniversary!!!....The Day...    Mild_Pos   \n",
      "1       @McfarlaneGlenda Happy Anniversary!!!....The D...    Mild_Pos   \n",
      "2       @thevivafrei @JustinTrudeau Happy Anniversary!...    Mild_Pos   \n",
      "3       @NChartierET Happy Anniversary!!!....The Day t...    Mild_Pos   \n",
      "4       @tabithapeters05 Happy Anniversary!!!....The D...    Mild_Pos   \n",
      "...                                                   ...         ...   \n",
      "451292  Outrageous :humanitarian aid convoy denied ent...  Strong_Neg   \n",
      "451295  RT: @netraKL RT @juanajaafar: you can count on...  Strong_Pos   \n",
      "451296  RT @juanajaafar: you can count on Israel and i...  Strong_Pos   \n",
      "451307  .@rebeccay #GFM is Gaza Freedom March, a convo...  Strong_Pos   \n",
      "451308  @rebeccay #GFM is Gaza Freedom March, a convoy...  Strong_Pos   \n",
      "\n",
      "                                              clean_tweet  \n",
      "0       happy anniversary the day the freedumb died in...  \n",
      "1       happy anniversary the day the freedumb died in...  \n",
      "2       happy anniversary the day the freedumb died in...  \n",
      "3       happy anniversary the day the freedumb died in...  \n",
      "4       happy anniversary the day the freedumb died in...  \n",
      "...                                                   ...  \n",
      "451292  outrageous humanitarian aid convoy denied entr...  \n",
      "451295  you can count on israel and its arab muslim fr...  \n",
      "451296  you can count on israel and its arab muslim fr...  \n",
      "451307  gf m is gaza freedom march a convoy going gaza...  \n",
      "451308  gf m is gaza freedom march a convoy going gaza...  \n",
      "\n",
      "[97002 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Filter the DataFrame to show only the duplicate rows based on 'clean_tweet' and 'Sentiment' columns\n",
    "duplicate_rows = df[df.duplicated(subset=['clean_tweet', 'Sentiment'], keep=False)]\n",
    "\n",
    "# Display the duplicate rows\n",
    "print(duplicate_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    Tweet   Sentiment  \\\n",
      "0       @_angelica_toy Happy Anniversary!!!....The Day...    Mild_Pos   \n",
      "8       Freedom Convoy as InkBlot Test https://t.co/au...  Strong_Pos   \n",
      "13      @mark_slapinski Well itâs pretty easy to see...  Strong_Pos   \n",
      "23      @JustinTrudeau You Belong In Jail.\\n#VaccineMa...     Neutral   \n",
      "25      #FreeDumbConvoy #FreedomConvoy #Freedumbers #f...     Neutral   \n",
      "...                                                   ...         ...   \n",
      "451327  Gaza; Peace n' Freedom - Viva Palestina convoy...  Strong_Pos   \n",
      "451328  Face of Defense: Soldier Finds Freedom in U.S....  Strong_Pos   \n",
      "451329  Face of Defense: Soldier Finds Freedom in U.S....  Strong_Pos   \n",
      "451330  Gaza; Peace n' Freedom - \"Israel stops aid con...  Strong_Pos   \n",
      "451331             @convoy 83 yes! get on freedom server!  Strong_Pos   \n",
      "\n",
      "                                              clean_tweet  \n",
      "0       happy anniversary the day the freedumb died in...  \n",
      "8                          freedom convoy as inkblot test  \n",
      "13      well it s pretty easy to see what their agenda...  \n",
      "23      you belong in jail vaccine mandates crimes aga...  \n",
      "25      free dumb convoy freedom convoy free dumber s ...  \n",
      "...                                                   ...  \n",
      "451327  gaza peace and freedom viva palestina convoy e...  \n",
      "451328  face of defense soldier finds freedom in you s...  \n",
      "451329  face of defense soldier finds freedom in you s...  \n",
      "451330  gaza peace and freedom israel stops aid convoy...  \n",
      "451331                          yes get on freedom server  \n",
      "\n",
      "[373889 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate rows based on 'clean_tweet' and 'Sentiment' columns\n",
    "df = df.drop_duplicates(subset=['clean_tweet', 'Sentiment'], keep='first')\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(373889, 3)"
      ]
     },
     "execution_count": 570,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_tweet'] = df['clean_tweet'].str.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#removing stopwords\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop = set(stopwords.words('english'))\n",
    "df['clean_tweet'] = df['clean_tweet'].apply(lambda x: [word for word in x if word not in stop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@_angelica_toy Happy Anniversary!!!....The Day...</td>\n",
       "      <td>Mild_Pos</td>\n",
       "      <td>[happy, anniversary, day, freedumb, died, tune...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Freedom Convoy as InkBlot Test https://t.co/au...</td>\n",
       "      <td>Strong_Pos</td>\n",
       "      <td>[freedom, convoy, inkblot, test]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>@mark_slapinski Well itâs pretty easy to see...</td>\n",
       "      <td>Strong_Pos</td>\n",
       "      <td>[well, pretty, easy, see, agenda, pierre, rema...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>@JustinTrudeau You Belong In Jail.\\n#VaccineMa...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>[belong, jail, vaccine, mandates, crimes, huma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>#FreeDumbConvoy #FreedomConvoy #Freedumbers #f...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>[free, dumb, convoy, freedom, convoy, free, du...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Tweet   Sentiment  \\\n",
       "0   @_angelica_toy Happy Anniversary!!!....The Day...    Mild_Pos   \n",
       "8   Freedom Convoy as InkBlot Test https://t.co/au...  Strong_Pos   \n",
       "13  @mark_slapinski Well itâs pretty easy to see...  Strong_Pos   \n",
       "23  @JustinTrudeau You Belong In Jail.\\n#VaccineMa...     Neutral   \n",
       "25  #FreeDumbConvoy #FreedomConvoy #Freedumbers #f...     Neutral   \n",
       "\n",
       "                                          clean_tweet  \n",
       "0   [happy, anniversary, day, freedumb, died, tune...  \n",
       "8                    [freedom, convoy, inkblot, test]  \n",
       "13  [well, pretty, easy, see, agenda, pierre, rema...  \n",
       "23  [belong, jail, vaccine, mandates, crimes, huma...  \n",
       "25  [free, dumb, convoy, freedom, convoy, free, du...  "
      ]
     },
     "execution_count": 573,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download WordNet data\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize WordNet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define a function to lemmatize a list of words\n",
    "def lemmatize_words(words):\n",
    "    return [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "# Apply lemmatization to the 'clean_tweet' column\n",
    "df['clean_tweet'] = df['clean_tweet'].apply(lemmatize_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_tweet'] = df['clean_tweet'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean white spaces in text\n",
    "def clean_whitespace(text):\n",
    "    if isinstance(text, str):  # Check if the value is a string\n",
    "        # Remove leading and trailing spaces\n",
    "        cleaned_text = text.strip()\n",
    "        # Replace multiple spaces with a single space using regex\n",
    "        cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "        return cleaned_text\n",
    "    else:\n",
    "        return text  # Return non-string values as is\n",
    "\n",
    "# Apply whitespace cleaning function to all text columns in the DataFrame\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':  # Check if the column contains text data\n",
    "        df[col] = df[col].apply(clean_whitespace)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
